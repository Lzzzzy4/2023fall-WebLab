# Web实验报告
组员：
## 一、实验目的
豆瓣 `(www.douban.com)` 是一个中国知名的社区网站，以书影音起家，用户可以在豆瓣上查看感兴趣的电影、书籍、音乐等内容，还可以关注自己感兴趣的豆友。

本实验的目的为爬取指定的电影、书籍的主页，并解析其基本信息，然后结合给定的标签信息，实现电影和书籍的检索并评估其效果；在此基础上，结合用户的评价信息及用户间社交关系，进行个性化电影、书籍推荐。
## 二、实验环境
- 操作系统：Windows 11
- 开发环境：jupyter notebook
- 软件平台：visual studio code
## 三、实验内容
### 1.爬虫
#### 爬取数据：
下面以电影为例，说明具体爬取过程
```python
for line in fread.readlines():
    time.sleep(0.1) # 反爬

    id = line.strip('\n')

    #防止重爬 && 跳过下架电影的id
    if id in id_map or id in skip_set:
        continue

    print(id)

    url = 'https://movie.douban.com/subject/' + id + '/'
    try:
        content = get_content(url)
        id_map[id] = content
    except:
        with open(error_path, "a+") as ferror:
            ferror.write(id + "\n")
```
我们使用`request`库进行爬虫，通过维护`id_map`与`skip_set`两个集合，实现了断点续爬与跳过下架页面的功能。其中，id_map用于存储爬取到的电影信息，skip_set用于存储下架电影的id，避免重复爬取。`get_content`函数用于获取电影主页的html内容。
#### 解析数据：
以获取info信息为例，具体解析过程如下：
```python
def get_info(soup):
    info = soup.find('div', id='info')
    info = info.get_text().strip('\n').split('\n')
    info = [i.split(': ') for i in info]
    info_dict = {}
    for i in info:
        #check for i[1] exsit
        if (len(i) == 1):
            continue
        info_dict[i[0]] = i[1].split(' / ')
        if (len(info_dict[i[0]]) == 1):
            info_dict[i[0]] = info_dict[i[0]][0]
    return info_dict
```
我们使用`BeautifulSoup`库进行解析，通过`find`函数找到html中对应的标签，然后使用`get_text`函数获取标签内的文本内容，再通过`strip`函数去除多余的换行符，最后使用`split`函数进行分割，得到一个列表。我们将列表中的每一项再次使用`split`函数进行分割，得到一个二维列表。最后，我们将二维列表转化为字典，得到电影的info信息。

#### 反爬策略：
我们通过给`request`函数添加`headers`参数，模拟浏览器访问，避免被反爬。同时，为了避免同一ip在一定时间内访问次数过多，我们在每次爬取后都使用`time`库的`sleep`函数，使爬虫休眠0.1s。

#### 实验结果：
最终爬取的数据如`data/Book_info.json`和`data/Movie_info.json`所示。这里仅展示部分
```json
{
    "1046265": {
        "name": "挪威的森林",
        "info": {
            "原作名": "ノルウェイの森",
            "出版年": "2001-2",
            "页数": "350",
            "定价": "18.80元",
            "装帧": "平装",
            "ISBN": "9787532725694"
        },
        "rating": {
            "评分": " 8.1 ",
            "评分人数": "346906",
            "1星": "34.1%",
            "2星": "34.1%",
            "3星": "34.1%",
            "4星": "34.1%",
            "5星": "34.1%"
        },
        "intro": "这是一部动人心弦的、平缓舒雅的、略带感伤的恋爱小说。..."
    },
    ...
}
```
### 2.检索

#### 预处理

这里，我们采用结巴分词库，同时加入简介中的类型字段，并合并了近义词，删除了停用词，以帮助接下来的查询。过程如下所示：

```python
for id in content:
    if self.type == "Movie":# 书籍没有类型
        Type = content[id]['info']['类型']
    intro = content[id]['intro']
    seg_list = jieba.cut(intro)

    # 合并同义词 && 去除停用词

    seg_set = set()
    for seg in seg_list:
        if seg in synonym:
            seg_set.add(synonym[seg])
        elif seg not in stop_word:
            seg_set.add(seg)

    # merge seg_set and type
    if self.type == "Movie":
        for t in Type:
            seg_set.add(t)

    if id in tag_map:
        for tag in tag_map[id]:
            seg_set.add(tag)

    # output[id] = "/".join(seg_set)
    content[id]['tags'] = '/'.join(seg_set)
```

这里，我们将使用`jieba`库与`thuac`库分词的结果进行比较，最终选择使用`jieba`库。以对《肖申克的救赎》电影的分词结果为例，`jieba`库分词结果如下：
```json
"tags": "第一/使用/消失/一把/虚伪/金球奖/总帐/合法/TimRobbins/妻/囚禁/知识/击退/革命家/到期/愿望/风浪/谋杀罪/误杀/出/正义/现身/注意/几十年/相等/阴险/做/监狱/暗中/提名/血案/人数/洗雪/女明星/偷税/其才/弗里/大显/狱长/帮助/广告/没/越狱/一幅/犯罪/经济/石锤/脱离/躲藏/渐成/自己/窃贼/夜间/重视/遭受/翻案/多项/探悉/和睦/能够/年成/控告/彻底/罗宾斯/以/曼/担负/地奔/一度/释放/任意/导致/灰心/连同/一场/取得/扮演/难友/蒂姆/旧交/水星/救赎/计划/肖申克/燃起/向来/1995/答/假装/入狱/跟/初/针对/瑞德/受冤/情人/吗/下级/10/正在/偶然/证明/如果/派遣/杀死/引起/异/MorganFreeman/快/激发/确实/被/道格拉斯/和/电闪雷鸣/摩根/领队/一次/唯一/安迪/公/重获/件/找出/厘/奖励/长兄/就让/两人/一生/书籍/典狱长/一名/勇敢/本片/连/剧情/喽/少"
```
`thuac`库分词结果如下：
```json
"tags": "厘/以/件/确实/宾斯TimRobbins/夜间/任意/跟/一度/派遣/虚伪/受冤/和/向来/勇敢/提名/窃贼/年成/出/救赎/狱长/释放/情人/蒂姆/偶然/即使/妻/被/吗/下级/敝/官避税/肖申克/旧交/土星奖/广告/引起/宽度/燃起/导致/道格拉斯/长兄/典狱/本片/做/注意/扮演/计划/风浪/相同/电闪雷鸣/担负/入狱/彻底/没/喽/针对/越狱/10/找出/女人/书籍/长洗/洗雪/重视/领队/激发/连同/探悉/答/石锤/叫做/使用/紧靠/剧情/囚禁/几十/摩根•/境地/集市/如果/帮助/躲藏/相等/异/犯罪/增长/能够/消失/第一/初/正在/一生/非常/大多/总帐/正义/安迪/翻案/脱谋/合法/人数/1995年/赶快/经济/杀死/银行/连/取得/现身/弗里曼MorganFreeman饰/证明/难友/少/逐渐/自己/假装/唯一/球体/杀罪/击退/线/暗中/再/血案/瑞德/和睦/向/钱财/成为/监狱/坏/明星/控告/知识/奖励/误杀/遭受/到期/大显其才/灰心/阴险/愿望"
```
可以看到，`jieba`库对外国人名的分词更加自然，同时更好地呈现了年份的分词结果，所以我们选择使用`jieba`库。

#### 倒排表与布尔查询

这里，我们实现了多层跳表，以及对应的插入、删除、查询、合并等操作，详情参见`src/skip_list.py`

我们还通过实现带符号栈的表达式求值模块，实现了布尔查询的功能，详情参见`src/expression.py`

通过调节多层跳表的`level`值，我们测试了它与链表、一层跳表的性能差异，具体如下表所示。其中，`setup time`代表建立跳表所需的时间，`query time`代表跳表查询布尔表达式`爱情 and 剧情`所需的时间。

|                  | 链表   | 一层跳表 | 多层跳表 | 
| -------------    | ------ | -------  | -------- |
| $setup\ time/s$  |1.674   | 1.803    |  2.482   | 
| $query\ time/ms$ | 103    | 96       | 60       | 

可以看到，随着跳表的层数增多，建立跳表所需的时间越来越长，但是单词查询的时间越来越短。且多层跳表相较于链表的性能提升接近一倍。

#### 索引压缩

我们实现了按块存储和前缀压缩两种方式。针对id进行压缩，按块压缩默认一块包含五个条目。我们以查询书籍中包含"爱情"标签的用例进行比较。其中，$dict$代表使用python内置的平衡树实现，$block$代表使用按块压缩，$trie_1$代表仅对id进行前缀压缩，$trie_2$代表对id进行前缀压缩，并且对条目内的标签也进行前缀压缩。

|               | $dict$ | $block$ | $trie_1$ | $tire_2$ |
| ------------- | ------ | ------- | -------- | -------- |
| $query\ time/ms$ | 140    | 130     | 120      | 109      |
| $memory/kB$   | 10968  | 11936   | 22732    | 35560    |

对于时间分析。按块压缩可以将总条目数n减小5倍，块内查询即为遍历所有的块，但时间开销较小，所以有一定的优化提升。前缀压缩，由于trie树查询的时间复杂度为$O(n)$，优于dict查询的时间复杂度$O(nlog(n))$，提升较为明显。将id条目中标签进一步压缩为trie则可以进一步提升。

对于空间分析。按块压缩中，我们需要将原本的id作为信息添加到每一个条目中，所以需要更多的内存空间。同时，按块压缩要求每一个词项占内存空间较小，可以从bit位节省空间；但是每一项书籍信息占用空间很大，对于id的压缩无法做到在bit位上节省空间。前缀压缩中，由于trie树的实现较为复杂，包含更多指针，所以需要用到更大的内存空间，并且对于id压缩使用的是数字，对于标签压缩使用的是汉字，相对英文字母有一定的劣势。

两种索引压缩在内存上表现得不理想，主要因为书籍信息的存储结构与词典略有不同，难以发挥两者的优势。

### 3.推荐